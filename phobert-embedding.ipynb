{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# BATCH_SIZE = 100\nSEED = 61\n\nimport re\nimport time\nimport string\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n# import tensorflow_datasets as tfds\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n# tf.keras.utils.set_random_seed(SEED)\n\n# from tensorflow.keras.utils import to_categorical\n# from tensorflow.keras.models import Model\n# from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Flatten\n# from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n# from tensorflow.keras import layers, Sequential\n\n# from gensim.models.fasttext import FastText\n# from gensim.models import Word2Vec\n\n# import fasttext\n# import fasttext.util\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T02:30:08.612050Z","iopub.execute_input":"2022-05-28T02:30:08.612478Z","iopub.status.idle":"2022-05-28T02:30:10.085452Z","shell.execute_reply.started":"2022-05-28T02:30:08.612446Z","shell.execute_reply":"2022-05-28T02:30:10.084509Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_path = '../input/cleaned-sentiment-text/train.csv'#'data/cleaned/train.csv'\nvalidation_path = '../input/cleaned-sentiment-text/validation.csv'#'data/cleaned/validation.csv'\ntest_path = '../input/cleaned-sentiment-text/test.csv'#'data/cleaned/test.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:21:27.561062Z","iopub.execute_input":"2022-05-28T02:21:27.561802Z","iopub.status.idle":"2022-05-28T02:21:27.567376Z","shell.execute_reply.started":"2022-05-28T02:21:27.561764Z","shell.execute_reply":"2022-05-28T02:21:27.566607Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# data is already cleaned and splitted in 3 parts: train, validation and test\ntrain_df = pd.read_csv(train_path, index_col=0)\nvalidation_df = pd.read_csv(validation_path, index_col=0)\ntest_df = pd.read_csv(test_path, index_col=0)\nNUM_CLASSES = len(train_df['label'].unique())\nindex2class = {0:'neg', 1:'pos'}\nclass2index = {'neg': 0, 'pos':1}\n\n# X\ntrain_data = train_df.iloc[:, 0].to_numpy()\nvalidation_data = validation_df.iloc[:, 0].to_numpy()\ntest_data = test_df.iloc[:, 0].to_numpy()\nwhole_data = np.concatenate((train_data, validation_data, test_data), dtype=object)\n\n# y\nlabel_train = np.array([class2index[i] for i in train_df.iloc[:, 1]])\nlabel_validation = np.array([class2index[i] for i in validation_df.iloc[:, 1]])\nlabel_test = np.array([class2index[i] for i in test_df.iloc[:, 1]])\nlabel_whole = np.concatenate((label_train, label_validation, label_test))\n\nprint('Number of train data:', train_df.shape[0])\nprint('Number of validation data:', validation_df.shape[0])\nprint('Number of test data:', test_df.shape[0])\nprint('Number of classes:', NUM_CLASSES, '->', train_df['label'].unique())\n\nMAXLEN = max([len(x.split()) for x in whole_data]) # = 492\nEMBEDDING_DIM = 100\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:21:27.568585Z","iopub.execute_input":"2022-05-28T02:21:27.569584Z","iopub.status.idle":"2022-05-28T02:21:28.596875Z","shell.execute_reply.started":"2022-05-28T02:21:27.569545Z","shell.execute_reply":"2022-05-28T02:21:28.595885Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of train data: 29744\nNumber of validation data: 9920\nNumber of test data: 9920\nNumber of classes: 2 -> ['neg' 'pos']\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                text label\n0  hôm_nay đi ngang quán quyết_định ghé mua quán ...   neg\n1  đến_súp lơ trưa nắng gắt chủ_nhật hy_vọng súp ...   pos\n2  hôm_qua xe đông khiếp lo bàn may_sao bàn lầu l...   pos\n3  món ăn_ở ngon món bò kho chủ quán quán phục_vụ...   pos\n4  đẹp trời đi mỳ mỳ vô_cùng mặn sợi bở bàn phục_...   neg","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>hôm_nay đi ngang quán quyết_định ghé mua quán ...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>đến_súp lơ trưa nắng gắt chủ_nhật hy_vọng súp ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hôm_qua xe đông khiếp lo bàn may_sao bàn lầu l...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>món ăn_ở ngon món bò kho chủ quán quán phục_vụ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>đẹp trời đi mỳ mỳ vô_cùng mặn sợi bở bàn phục_...</td>\n      <td>neg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def load_bert():\n    v_phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n    v_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n    return v_phobert, v_tokenizer\nphobert, tokenizer = load_bert()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:21:28.598677Z","iopub.execute_input":"2022-05-28T02:21:28.599318Z","iopub.status.idle":"2022-05-28T02:21:51.059445Z","shell.execute_reply.started":"2022-05-28T02:21:28.599274Z","shell.execute_reply":"2022-05-28T02:21:51.058316Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f7dadd71f4402eb7b4eada2e44615f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/518M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7722e7e4d4844b49b6384ae82f90d8f2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/874k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e47e1ae8f2e74f84888b12fb39993bbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f794c96e234a07b94eb51e76edd2b0"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LENGTH = 256 # phobert default max sequence length\ndef phobert_tokenize(data):\n    tokenized_data = []\n    for line in data:\n        tokenenized_line = tokenizer.encode(line, max_length=256, truncation=True)\n        tokenized_data.append(tokenenized_line)\n    return tokenized_data","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:39:19.184368Z","iopub.execute_input":"2022-05-28T02:39:19.184789Z","iopub.status.idle":"2022-05-28T02:39:19.191300Z","shell.execute_reply.started":"2022-05-28T02:39:19.184756Z","shell.execute_reply":"2022-05-28T02:39:19.190248Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# tokenize input sentence using phobert tokenizer\ntokenized_train = phobert_tokenize(train_data)\ntokenized_validation = phobert_tokenize(validation_data)\ntokenized_test = phobert_tokenize(test_data)\n# tokenized_whole = np.concatenate((tokenized_train, tokenized_validation, tokenized_test), dtype=object)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:39:23.662572Z","iopub.execute_input":"2022-05-28T02:39:23.662956Z","iopub.status.idle":"2022-05-28T02:39:42.990002Z","shell.execute_reply.started":"2022-05-28T02:39:23.662926Z","shell.execute_reply":"2022-05-28T02:39:42.988994Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# pad sentence to a pre-defined max length\n# no need truncating since it is already truncated in the phobert tokenizing process\npadded_train = pad_sequences(tokenized_train, maxlen=MAXLEN, padding='post')\npadded_validation = pad_sequences(tokenized_validation, maxlen=MAXLEN, padding='post')\npadded_test = pad_sequences(tokenized_test, maxlen=MAXLEN, padding='post')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:39:50.286044Z","iopub.execute_input":"2022-05-28T02:39:50.286465Z","iopub.status.idle":"2022-05-28T02:39:50.755171Z","shell.execute_reply.started":"2022-05-28T02:39:50.286429Z","shell.execute_reply":"2022-05-28T02:39:50.754101Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# np.array([[0.1, 0.15],\n#                  [0.2, 0.25],\n#                  [0.3, 0.35],\n#                  [0.4, 0.45],\n#                  [0.5, 0.55],\n#                  [0.6, 0.65],\n#                  [0.7, 0.75],\n#                  [0.8, 0.85],\n#                  [0.9, 0.95],\n#                  [1.0, 1.5]]).reshape(10,2,1)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T02:44:12.301266Z","iopub.execute_input":"2022-05-28T02:44:12.301716Z","iopub.status.idle":"2022-05-28T02:44:12.306725Z","shell.execute_reply.started":"2022-05-28T02:44:12.301674Z","shell.execute_reply":"2022-05-28T02:44:12.305563Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# ref: https://miai.vn/2020/12/29/bert-series-chuong-3-thu-nhan-dien-cam-xuc-van-ban-tieng-viet-voi-phobert-cach-1/","metadata":{},"execution_count":null,"outputs":[]}]}